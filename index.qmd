---
title: "thesctrading"
format:
  html:
    code-fold: true
    df-print: paged

jupyter: py312env

execute:
  echo: false
  warning: false
  message: false
---

## S&C Trading Strategy Overview

### 1. Strategy Outline

Our team is implementing a buy low / sell high strategy focused on short-term momentum trading in TSLA stock. We aim to take advantage of periods of elevated volatility and momentum.
We use implied volatility from options data to inform our volatility expectation and therefore our limit sell (or buy in the case of a short).
We also use probabilities from a logistic regression based on alternative and technical to size our trades, a key differentiation to our HW1 model.
Our model also has more thoughtful entry parameters and trades more selectively based on underlying conditions in the stock.
We walk through our model and an example trade below.

### 2. Example Trade Walkthrough

Pre-Trade Filter

1. Check if job postings for software roles increased week-over-week
   (as well as other FRED/alternative data including pairs performance)
2. Check if implied volatility (IV) percentile > 40% and < 90%
3. If both are true → proceed to trade evaluation and if price > last weeks close, short the stock and vice versa

Position Sizing

We run a logistic regression on the success of our historical trades with IV, SMA, and WAP as features to determine how to size our new positions.
Given our limited dataset, this is only evident in a few of our trades (as a result of our fairly strict pre-trade criteria), but allows us to manage risk
under various momentum regimes, taking discretionary risk in favorabe market conditions and conserving capital in those which are unfavorable.

Exit Strategy

We use two exit triggers to manage risk and lock in profits:

1. If position is open for more than 5 trading days (one week), exit regardless of performance
2. If position hits limit price (determined by options implied volatility expectations for the week), sell

*note that compared to the basic HW1 empirical volatility model, we forecasted vol using the options implied volatility.
We converted the first IV datapoint of a trade period (ie. Monday's open) to an expectation of weekly vol and used this vol to
calculate our limit buy and sells. We thought this was more descriptive and informative of market expectations for upcoming vol.
We also used IV in our pre-trade checklist in order to ensure we are taking risk at the right times. We considered using ML to determine
more effective pre-trade requirements which were less binary, but did not implement these. Using the IV is much better at being **forward-looking**
as opposed to the previous backward looking volatility analysis. This helps us build a more informed, sensible, and risk-aware model.*

### 3. Data Requirements

Data from the following sources was used to inform our trades:

- Price data and technical indicators (SMA, WAP, IV)
- Job posting data & other relevant metrics from FRED
- Implied volatility percentile data (via IBKR)

---

### 4. Commentary & Reflection

There were a few features we had hoped to include in our model which proved unsuccessful or were too cumbersome to implement.
One such feature was determining which direction (long or short) to trade based on whether the price was above the 50-day SMA as opposed to the prior period's close did not work as well as we had hoped from a returns perspective.
We thought it would be a smoother metric than previous trade period close. However, after testing the strategy using SMA for several period lengths (20, 30, etc.), we found that the results were worse.
We also found that using implied volatility was much more effective and that sizing positions continuously through the logistic regression
was very effective relative to the standard model. We can infer that this makes our model more well informed and therefore more effective.
With more time, we would've implemented more fundamental metrics to help inform our trading plan (historical average EV/EBITDA and other valuation metrics).


### 5. FRED Data: Software Developer Job Postings

The below is demonstrating our ability to use the FRED API to pull in features. This example is weekly software job postings which we use as a pre-trade filter in our model.

```{python}
import pandas as pd
import matplotlib.pyplot as plt
from fredapi import Fred

# trade_period function
def get_trade_period(dt):
    iso_year, iso_week, _ = dt.isocalendar()
    return float(f"{iso_year}.{iso_week:02d}")

# Connect to FRED
fred = Fred(api_key="1c00931ee7dc4304c6bb68b72fb2d68f")

# Fetch data
series_id = "IHLIDXUSTPSOFTDEVE"
data = fred.get_series(series_id)

# Convert to DataFrame
df = pd.DataFrame(data, columns=["Job Postings"])
df.index.name = "Date"
df = df.reset_index()
df["Date"] = pd.to_datetime(df["Date"])
df["trd_prd"] = df["Date"].apply(get_trade_period)

# collapse to one val per week and compare to prior week
weekly_jobs    = df.groupby("trd_prd")["Job Postings"].last().sort_index()
job_diff       = weekly_jobs.diff()
job_increased  = job_diff > 0

#print(job_increased.head(10))
# Display last 5 rows as table
#df.tail()
```

```{python}
# Plot the time series
import plotly.express as px
fig = px.line(
    df,
    x="Date",
    y="Job Postings",
    title="US Software Developer Job Postings Index",
    markers=True
)

fig.update_layout(
    yaxis_title="Index Value",
    hovermode="x unified",
    xaxis=dict(
        title="Date",
        rangeselector=dict(
            buttons=list([
                dict(count=1, label="1m",  step="month", stepmode="backward"),
                dict(count=3, label="3m",  step="month", stepmode="backward"),
                dict(count=6, label="6m",  step="month", stepmode="backward"),
                dict(count=1, label="YTD", step="year",  stepmode="todate"),
                dict(step="all")
            ])
        ),
        rangeslider=dict(visible=True),
        type="date"
    )
)

fig.show()
```

# Blotter & Ledger


```{python}

import numpy as np
import pandas as pd
import shinybroker as sb
import datetime
import matplotlib.pyplot as plt
import statsmodels.api as sm
```


```{python}
asset= sb.Contract({
    'symbol': "TSLA",
    'secType': "STK",
    'exchange': "SMART",
    'currency': "USD"
})

benchmark = sb.Contract({
    'symbol': "SPX",
    'secType': "IND",
    'exchange': "CBOE",
    'currency': "USD"
})

benchmark_fetch = sb.fetch_historical_data(
    contract=benchmark,
    endDateTime='',          # now
    durationStr='1 Y',       # past year
    barSizeSetting='1 day',  # daily bars
    whatToShow='TRADES',
    useRTH=True,
)
benchmark = benchmark_fetch['hst_dta']

#### Get hourly data to use for calculating vol
historical_data_hourly_fetch = sb.fetch_historical_data(
    contract=asset,
    endDateTime='',         # Let IBKR set the "now" time
    durationStr='1 Y',      # Past 1 year
    barSizeSetting='1 hour', # Daily bars
    whatToShow='TRADES',
    useRTH=True,
    #date_format=1,          # String time zone date
    #keepUpToDate=False
)
historical_data_hourly = historical_data_hourly_fetch['hst_dta']

#### Get daily data as well because it speeds up the code
####   writing process.
historical_data_daily_fetch = sb.fetch_historical_data(
    contract=asset,
    endDateTime='',         # Let IBKR set the "now" time
    durationStr='1 Y',      # Past 1 year
    barSizeSetting='1 day', # Daily bars
    whatToShow='TRADES',
    useRTH=True,
    #date_format=1,          # String time zone date
    #keepUpToDate=False
)
historical_data_daily = historical_data_daily_fetch['hst_dta']
# print("HDD", historical_data_daily)

#### Fetch your liquid trading hours for the asset
#### You'll need this later!
ares_deets = sb.fetch_contract_details(
    contract=sb.Contract({
        'symbol': "TSLA",
        'secType': "STK",
        'exchange': "SMART",
        'currency': "USD"
    })
)
liquid_hours = ares_deets['liquidHours']
# print(liquid_hours)
#liquid_hours = (ares_deets['liquidHours'][0])

def safe_fetch_iv(contract, durationStr, barSizeSetting, label):
    try:
        fetch = sb.fetch_historical_data(
            contract=contract,
            endDateTime='',
            durationStr=durationStr,
            barSizeSetting=barSizeSetting,
            whatToShow='OPTION_IMPLIED_VOLATILITY',
            useRTH=True,
        )
        df = fetch['hst_dta']
        return df
    except Exception as e:
#        print(f"{label} IV fetch failed: {e}")
        return pd.DataFrame()

# Replace your fetch calls with:
#iv_historical_data_hourly = safe_fetch_iv(
#    contract=asset,
#    durationStr='1 Y',
#    barSizeSetting='1 hour',
#    label="Hourly"
#)

iv_historical_data_daily = safe_fetch_iv(
    contract=asset,
    durationStr='1 Y',
    barSizeSetting='1 day',
    label="Daily"
)


historical_data_daily_fetch_SMA = sb.fetch_historical_data(
    contract=asset,
    endDateTime='',         # Let IBKR set the "now" time
    durationStr='2 Y',      # Past 1 year
    barSizeSetting='1 day', # Daily bars
    whatToShow='TRADES',
    useRTH=True,
    #date_format=1,          # String time zone date
    #keepUpToDate=False
)
historical_data_daily_SMA = historical_data_daily_fetch_SMA['hst_dta']#%% md
```


```{python}
import pandas as pd

# Make sure 'date' is a datetime column
historical_data_daily_SMA['timestamp'] = pd.to_datetime(historical_data_daily_SMA['timestamp'])

# Sort by date just to be safe
historical_data_daily_SMA = historical_data_daily_SMA.sort_values('timestamp')

# Calculate 30-day simple moving average
historical_data_daily_SMA['SMA_30'] = historical_data_daily_SMA['close'].rolling(window=30).mean()

#print(historical_data_daily_SMA['SMA_30'])

#### Prepare Data
# Function to calculate trade period
def get_trade_period(dt):
    iso_year, iso_week, _ = dt.isocalendar()
    return float(f"{iso_year}.{iso_week:02d}")

# Ensure 'date' column is in datetime format
historical_data_daily['timestamp'] = pd.to_datetime(historical_data_daily['timestamp'])
historical_data_hourly['timestamp'] = pd.to_datetime(historical_data_hourly['timestamp'])
iv_historical_data_daily['timestamp'] = pd.to_datetime(iv_historical_data_daily['timestamp'])
#iv_historical_data_hourly['timestamp'] = pd.to_datetime(iv_historical_data_hourly['timestamp'])
#liquid_hours['timestamp'] = pd.to_datetime(liquid_hours['timestamp'])

# Apply trade period calculation
historical_data_daily['trd_prd'] = historical_data_daily['timestamp'].apply(get_trade_period)
historical_data_hourly['trd_prd'] = historical_data_hourly['timestamp'].apply(get_trade_period)
iv_historical_data_daily['trd_prd'] = iv_historical_data_daily['timestamp'].apply(get_trade_period)
#iv_historical_data_hourly['trd_prd'] = iv_historical_data_hourly['timestamp'].apply(get_trade_period)
historical_data_daily_SMA['trd_prd'] = historical_data_daily_SMA['timestamp'].apply(get_trade_period)

#liquid_hours['trd_prd'] = liquid_hours['timestamp'].apply(get_trade_period)

# Identify the first full five-trading-day week
weekly_counts = historical_data_daily.groupby('trd_prd')['timestamp'].count()
first_full_week = weekly_counts[weekly_counts >= 5].index.min()

# Filter historical_data_daily
historical_data_daily = historical_data_daily[historical_data_daily['trd_prd'] >= first_full_week]
```

```{python}
# Print dataframes
# Commented out these prints for now

#print("Historical Data Daily:")
#print(historical_data_daily)

#print("\nHistorical Data Hourly:")
#print(historical_data_hourly)

#print("IV Historical Data Daily:")
#print(iv_historical_data_daily)

#print("\n IV Historical Data Hourly:")
#print(iv_historical_data_daily)

#print("Historical SMA Data Daily:")
#print(historical_data_daily_SMA)

#print("\nLiquid Hours:")
#print(liquid_hours)


```

```{python}
# Calc Obs & Exp Vol
# Extract trade periods from daily historical data
trade_periods = historical_data_daily['trd_prd'].unique()

# Calculate observed volatility using hourly data
hourly_log_returns = np.log(historical_data_hourly['close'] / historical_data_hourly['close'].shift(1))
hourly_vols = hourly_log_returns.groupby(historical_data_hourly['timestamp'].dt.strftime('%Y-%W')).std()

# Convert hourly vol to weekly vol (scale by sqrt(32.5))
weekly_vols = hourly_vols * np.sqrt(32.5)

# Create DataFrame
vol_calcs = pd.DataFrame(index=trade_periods, columns=['obs_vol', 'exp_vol'])
vol_calcs['obs_vol'] = weekly_vols.values[:len(trade_periods)]

# Set expected vol (shifted obs_vol)
vol_calcs['exp_vol'] = vol_calcs['obs_vol'].shift(1)

# Display the DataFrame
# print("\nVolatility Calculations:")
# print(vol_calcs)

# Calc Obs & Exp Vol
# Extract trade periods from daily historical data
trade_periods = historical_data_daily['trd_prd'].unique()

# Calculate observed volatility using hourly data
hourly_log_returns = np.log(historical_data_hourly['close'] / historical_data_hourly['close'].shift(1))
hourly_vols = hourly_log_returns.groupby(historical_data_hourly['timestamp'].dt.strftime('%Y-%W')).std()

# Convert hourly vol to weekly vol (scale by sqrt(32.5))
weekly_vols = hourly_vols * np.sqrt(32.5)

# Create DataFrame
smart_vol_calcs = pd.DataFrame(index=trade_periods, columns=['obs_vol', 'exp_vol'])
smart_vol_calcs['obs_vol'] = weekly_vols.values[:len(trade_periods)]

# Set expected vol (from options IV)
for trd_prd in smart_vol_calcs.index:
    smart_vol_calcs.at[trd_prd, 'exp_vol'] = iv_historical_data_daily.loc[
        iv_historical_data_daily['trd_prd'] == trd_prd, 'open'
    ].iloc[0] * np.sqrt(1/52)

# Display the DataFrame
#print("\nVolatility Calculations:")
#print(smart_vol_calcs)
vol_calcs = smart_vol_calcs

#vol_calcs = smart_vol_calcs # manually toggle active vol calc
```

```{python}
# Extract trade periods from daily historical data
trade_periods = historical_data_daily['trd_prd'].unique()

# Calc blotter
blotter = pd.DataFrame(index=trade_periods[1:], columns=['entry_timestamp', 'qty', 'exit_timestamp', 'entry_price', 'exit_price', 'success', 'iv', 'wap', 'sma'])
blotter[:] = None  # Set empty values
#print("\nBlotter:")
#print(blotter)

# Initialize Ledger
filtered_historical = historical_data_daily[historical_data_daily['trd_prd'] != historical_data_daily['trd_prd'].iloc[0]]
ledger = pd.DataFrame()
ledger['date'] = filtered_historical['timestamp']
ledger['position'] = 0.0  # Placeholder values
ledger['cash'] = 0.0  # Placeholder values
ledger['mark'] = historical_data_daily['close'][1:]
ledger['mkt_value'] = 0.0  # Placeholder values
#ledger['cash'] = 50000

#print("\nLedger:")
#print(ledger)
```

```{python}
# create function for IV percentile
def calc_iv_percentile_series(iv_df, window=252):
    iv_percentiles = {}
    iv_df_sorted = iv_df.sort_values("timestamp")

    for i, row in iv_df_sorted.iterrows():
        trd_prd = row['trd_prd']
        curr_iv = row['open']
        recent_ivs = iv_df_sorted[iv_df_sorted['timestamp'] < row['timestamp']].tail(window)['open']

        if len(recent_ivs) >= 10:
            percentile = (recent_ivs < curr_iv).mean() * 100
            iv_percentiles[trd_prd] = percentile

    return iv_percentiles

# calc percentiles for each trade period...
# want to only trade if IV percentile is >50% and <90%
iv_percentiles_by_trdprd = calc_iv_percentile_series(iv_historical_data_daily)

# shift dict to use previous iv percentile
prev_iv_percentiles = {}
sorted_keys = sorted(iv_percentiles_by_trdprd.keys())
for i in range(1, len(sorted_keys)):
    curr = sorted_keys[i]
    prev = sorted_keys[i - 1]
    prev_iv_percentiles[curr] = iv_percentiles_by_trdprd[prev]

# Blotter & Ledger Loop
# This is where "backtesting" really occurs. We're calculating the blotter &
# ledger that our trading system WOULD have produced.
for trd_prd in blotter.index:
    entry_timestamp = historical_data_hourly.loc[historical_data_hourly['trd_prd'] == trd_prd, 'timestamp'].iloc[0]
    entry_price = historical_data_hourly.loc[historical_data_hourly['trd_prd'] == trd_prd, 'open'].iloc[0]
    prev_close = historical_data_daily.loc[historical_data_daily['trd_prd'] < trd_prd, 'close'].iloc[-1]
    exp_vol = vol_calcs.loc[trd_prd, 'exp_vol']
    iv = iv_historical_data_daily.loc[iv_historical_data_daily['trd_prd'] == trd_prd, 'wap'].iloc[0]
    wap = historical_data_hourly.loc[historical_data_hourly['trd_prd'] == trd_prd, 'wap'].iloc[0]
    sma = historical_data_daily_SMA.loc[historical_data_daily_SMA['trd_prd'] == trd_prd, 'SMA_30'].iloc[0]
    prev_iv = prev_iv_percentiles.get(trd_prd, None)
    # INPUT PRE-TRADE FILTERS HERE

    if (not job_increased.get(trd_prd, False)
        or prev_iv is None
        or not (40 < prev_iv < 90)
    ):
        qty = 0
    elif entry_price > prev_close:
        qty = -100
        exit_price_strategy = entry_price * (1 - exp_vol)
    else:
        qty = 100
        exit_price_strategy = entry_price * (1 + exp_vol)

    period_data = historical_data_hourly[historical_data_hourly['trd_prd'] == trd_prd]
    max_high = period_data['high'].max()
    min_low = period_data['low'].min()

    if (qty > 0 and max_high >= exit_price_strategy) or (qty < 0 and min_low <= exit_price_strategy):
        success = True
        exit_price = exit_price_strategy

        if qty > 0:
            exit_timestamp = period_data.loc[period_data['high'] >= exit_price_strategy, 'timestamp'].iloc[0]
            exit_high_price = period_data.loc[period_data['high'] >= exit_price_strategy, 'high'].iloc[0]
            exit_low_price = period_data.loc[period_data['high'] >= exit_price_strategy, 'low'].iloc[0]
        else:
            exit_timestamp = period_data.loc[period_data['low'] <= exit_price_strategy, 'timestamp'].iloc[0]
            exit_high_price = period_data.loc[period_data['low'] <= exit_price_strategy, 'high'].iloc[0]
            exit_low_price = period_data.loc[period_data['low'] <= exit_price_strategy, 'low'].iloc[0]

        #print(exit_low_price, exit_high_price, exit_price_strategy, exit_timestamp, qty)
    elif qty == 0:
        success = False
        exit_price = entry_price  # or set to None if preferred
        exit_timestamp = entry_timestamp
    else:
        success = False
        exit_price = historical_data_daily.loc[historical_data_daily['trd_prd'] == trd_prd, 'close'].iloc[-1]
        #exit_timestamp = historical_data_daily.loc[historical_data_daily['trd_prd'] == trd_prd, 'timestamp'].iloc[-1]
        exit_timestamp = pd.to_datetime(
        historical_data_daily.loc[historical_data_daily['trd_prd'] == trd_prd, 'timestamp'].iloc[-1]).replace(hour=15, minute=0, second=0)


    blotter.loc[trd_prd] = [entry_timestamp, qty, exit_timestamp, entry_price, exit_price, success, iv, wap, sma]

    ledger.loc[ledger['date'] >= entry_timestamp, 'position'] += qty
    ledger.loc[ledger['date'] >= exit_timestamp, 'position'] -= qty
    ledger.loc[ledger['date'] >= entry_timestamp, 'cash'] -= qty * entry_price
    ledger.loc[ledger['date'] >= exit_timestamp, 'cash'] += qty * exit_price

# finally, calculate your strategy's end-of-day mark-to-market value
ledger['mkt_value'] = ledger['position'] * ledger['mark'] + ledger['cash']
```

```{python echo=false}
import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

X = blotter[['iv', 'wap', 'sma']]
y = blotter['success'].astype(int)

# Split train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
print(y.value_counts())


# Fit model
model = LogisticRegression()
model.fit(X_train, y_train)

# Predict
# y_pred = model.predict(X_test) # 0.3 threshold
probs = model.predict_proba(X_test)[:, 1]
y_pred = (probs > 0.3).astype(int)

fill = len(y_train)
padded_y_pred = np.concatenate(([1.0] * fill, y_pred))
padded_y_probs = np.concatenate(([1.0] * fill, probs))
blotter['prediction'] = pd.Series(padded_y_pred, index=blotter.index)
blotter['prediction %'] = pd.Series(padded_y_probs, index = blotter.index)

# Results
# REMOVING THESE TO CLEAN UP
#print("Coefficients:", model.coef_)
#print("Intercept:", model.intercept_)
#print(classification_report(y_test, y_pred))
#print("Blotter", blotter)
```
## Blotter & Ledger Results

### Blotter
```{python}
# Your final blotter DataFrame already populated at this point
import pandas as pd
from itables import show
blotter = blotter.round(3)  # Optional: round for cleaner display
# Display blotter as interactive table
show(blotter)
```


### Standard Ledger
```{python}
# Your final ledger DataFrame already populated at this point
ledger = ledger.round(2)  # Optional: clean output
# Display ledger as interactive table
show(ledger)
```



### NAV Results With Constant Quantity

```{python}
# Create an empty list to store categories
position_category = []

# Categorize positions manually
for pos in ledger['position']:
    if pos > 0:
        position_category.append('Long')
    elif pos < 0:
        position_category.append('Short')
    else:
        position_category.append('Cash Only')

# Add category column to ledger
ledger['position_category'] = position_category

# Define colors for each category
color_map = {'Long': 'green', 'Short': 'red', 'Cash Only': 'blue'}
colors = []

# Assign colors manually
for cat in ledger['position_category']:
    colors.append(color_map[cat])

# Plot NAV over time
plt.figure(figsize=(12, 6))
plt.scatter(ledger['date'], ledger['mkt_value'], c=colors, alpha=0.7)

plt.xlabel('Date')
plt.ylabel('Net Asset Value (NAV)')
plt.title('NAV Over Time with Position Coloring, Standard Model')
plt.grid(True)

# Manually add legend
for cat, color in color_map.items():
    plt.scatter([], [], color=color, label=cat)

plt.legend()
plt.show()
```

```{python}
# Initialize Ledger
filtered_historical = historical_data_daily[historical_data_daily['trd_prd'] != historical_data_daily['trd_prd'].iloc[0]]
new_ledger = pd.DataFrame()
new_ledger['date'] = filtered_historical['timestamp']
new_ledger['position'] = 0.0  # Placeholder values
new_ledger['cash'] = 0.0  # Placeholder values
new_ledger['mark'] = historical_data_daily['close'][1:]
new_ledger['mkt_value'] = 0.0  # Placeholder values
#new_ledger['cash'] = 50000

for trd_prd in blotter.index:

    qty = blotter.loc[trd_prd, 'qty'] * blotter.loc[trd_prd, 'prediction %']

    entry_price = blotter.loc[trd_prd]['entry_price']
    entry_timestamp = blotter.loc[trd_prd]['entry_timestamp']
    exit_timestamp = blotter.loc[trd_prd]['exit_timestamp']
    exit_price = blotter.loc[trd_prd]['exit_price']

    new_ledger.loc[new_ledger['date'] >= entry_timestamp, 'position'] += qty
    new_ledger.loc[new_ledger['date'] >= exit_timestamp, 'position'] -= qty
    new_ledger.loc[new_ledger['date'] >= entry_timestamp, 'cash'] -= qty * entry_price
    new_ledger.loc[new_ledger['date'] >= exit_timestamp, 'cash'] += qty * exit_price

# finally, calculate your strategy's end-of-day mark-to-market value
new_ledger['mkt_value'] = new_ledger['position'] * new_ledger['mark'] + new_ledger['cash']
#print(new_ledger)

# Create an empty list to store categories
position_category = []

# Categorize positions manually
for pos in new_ledger['position']:
    if pos > 0:
        position_category.append('Long')
    elif pos < 0:
        position_category.append('Short')
    else:
        position_category.append('Cash Only')

# Add category column to new_ledger
new_ledger['position_category'] = position_category

# Define colors for each category
color_map = {'Long': 'green', 'Short': 'red', 'Cash Only': 'blue'}
colors = []

# Assign colors manually
for cat in new_ledger['position_category']:
    colors.append(color_map[cat])
```
```{python}
# Your final ledger DataFrame already populated at this point
ledger = ledger.round(2)  # Optional: clean output
```
### Enhanced Ledger (continuous position sizing)
```{python}
# Display ledger as interactive table
show(ledger)
```
### NAV Results With Smart Quantity
```{python}
# Plot NAV over time
plt.figure(figsize=(12, 6))
plt.scatter(new_ledger['date'], new_ledger['mkt_value'], c=colors, alpha=0.7)

plt.xlabel('Date')
plt.ylabel('Net Asset Value (NAV)')
plt.title('NAV Over Time with Position Coloring, Enhanced Model')
plt.grid(True)

# Manually add legend
for cat, color in color_map.items():
    plt.scatter([], [], color=color, label=cat)

plt.legend()
plt.show()
```

```{python}
blotter['return'] = (((blotter['exit_price'] - blotter['entry_price']) / blotter['entry_price']) )* np.sign(blotter['qty'])# get returns irrespective of long/short

#print("\nExit Timestamps:")
#print(blotter['exit_timestamp'])

# Fetch entry and exit prices from historical_data_hourly based on timestamps
blotter['entry_price_underlying'] = blotter['entry_timestamp'].apply(
    lambda ts: historical_data_hourly.loc[historical_data_hourly['timestamp'] == ts, 'close'].iloc[0]
)

blotter['exit_price_underlying'] = blotter.apply(
    lambda row: (
        historical_data_hourly.loc[historical_data_hourly['timestamp'] == row['exit_timestamp'], 'close'].iloc[0]
        if not historical_data_hourly.loc[historical_data_hourly['timestamp'] == row['exit_timestamp'], 'close'].empty
        else (
            historical_data_daily.loc[historical_data_daily['timestamp'] == row['exit_timestamp'], 'close'].iloc[0]
            if not historical_data_daily.loc[historical_data_daily['timestamp'] == row['exit_timestamp'], 'close'].empty
            else None  # If neither dataset has the timestamp
        )
    ), axis=1
)


# Calculate return based on hourly data

blotter['return_underlying'] = (
    (blotter['exit_price_underlying'] - blotter['entry_price_underlying'])/ blotter['entry_price_underlying']) * np.sign(blotter['qty'])

x = pd.to_numeric(blotter['return_underlying'])
original_index = x.index.copy()
x.dropna(inplace=True)
dropped_rows = original_index.difference(x.index)
y = pd.to_numeric(blotter['return'])
y.drop(index=dropped_rows, inplace=True)

# Scatter plot of returns
plt.figure(figsize=(8, 6))
plt.scatter(x, y, alpha=0.6, label="Trades")
plt.xlabel("Underlying Return")
plt.ylabel("Strategy Return")
plt.title("Strategy Return vs. Underlying Return, Standard Model")

#print(x,y)

# Fit a linear regression model to get alpha and beta
x_with_const = sm.add_constant(x)  # Adds intercept for regression
model = sm.OLS(y, x_with_const).fit()
alpha, beta = model.params

# Plot regression line
plt.plot(x, alpha + beta * x, color='red', label=f"Regression Line (α={alpha:.4f}, β={beta:.4f})")
plt.legend()
plt.show()

# Print alpha and beta values

alpha, beta

blotter['return'] = ((blotter['exit_price'] - blotter['entry_price']) / blotter['entry_price']) * np.sign(blotter['qty'])

#print("\nExit Timestamps:")
#print(blotter['exit_timestamp'])

# Fetch entry and exit prices from historical_data_hourly based on timestamps
blotter['entry_price_underlying'] = blotter['entry_timestamp'].apply(
    lambda ts: historical_data_hourly.loc[historical_data_hourly['timestamp'] == ts, 'close'].iloc[0]
)

blotter['exit_price_underlying'] = blotter.apply(
    lambda row: (
        historical_data_hourly.loc[historical_data_hourly['timestamp'] == row['exit_timestamp'], 'close'].iloc[0]
        if not historical_data_hourly.loc[historical_data_hourly['timestamp'] == row['exit_timestamp'], 'close'].empty
        else (
            historical_data_daily.loc[historical_data_daily['timestamp'] == row['exit_timestamp'], 'close'].iloc[0]
            if not historical_data_daily.loc[historical_data_daily['timestamp'] == row['exit_timestamp'], 'close'].empty
            else None  # If neither dataset has the timestamp
        )
    ), axis=1
)

# Calculate return based on hourly data
blotter['return_underlying'] = ((blotter['exit_price_underlying'] - blotter['entry_price_underlying']) / blotter['entry_price_underlying'])* np.sign(blotter['qty'])

x = pd.to_numeric(blotter['return_underlying'])
original_index = x.index.copy()
x.dropna(inplace=True)
dropped_rows = original_index.difference(x.index)
y = pd.to_numeric(blotter['return'])
y.drop(index=dropped_rows, inplace=True)

# Scatter plot of returns
plt.figure(figsize=(8, 6))
plt.scatter(x, y, alpha=0.6, label="Trades")
plt.xlabel("Underlying Return")
plt.ylabel("Strategy Return")
plt.title("Strategy Return vs. Underlying Return, Enhanced Model")

#print(x,y)

# Fit a linear regression model to get alpha and beta
x_with_const = sm.add_constant(x)  # Adds intercept for regression
model = sm.OLS(y, x_with_const).fit()
alpha, beta = model.params

# Plot regression line
plt.plot(x, alpha + beta * x, color='red', label=f"Regression Line (α={alpha:.4f}, β={beta:.4f})")
plt.legend()
plt.show()

# Print alpha and beta values

alpha, beta
```
# Performance Statistics

At a high level, we compute:
- **Alpha & Beta** (vs SPX benchmark)
- **Annualized Volatility** of strategy returns
- **Geometric Mean Rate of Return** (annualized)
- **Sharpe Ratio** (assuming zero risk‐free rate)
- **Average return per trade**
- **Average number of trades per year**

```{python}

import numpy as np
import pandas as pd
import statsmodels.api as sm
from scipy.stats import gmean
from itables import show

# 1) make sure benchmark returns are up to date
benchmark['timestamp'] = pd.to_datetime(benchmark['timestamp'])
benchmark['trd_prd'] = benchmark['timestamp'].apply(get_trade_period)
period_spx = (
    benchmark
    .sort_values('timestamp')
    .groupby('trd_prd')['close']
    .agg(first='first', last='last')
)
period_spx['return'] = period_spx['last'] / period_spx['first'] - 1

# 2) extract our strategy's per‐period returns
strat_ret = blotter['return'].astype(float)
#print(strat_ret)

# 3) align and drop any NaNs
bench_ret = period_spx['return'].reindex(strat_ret.index)
df = pd.concat([strat_ret, bench_ret], axis=1, keys=['strat_ret','bench_ret']).dropna()

# 4) OLS
Y = df['strat_ret']
X = sm.add_constant(df['bench_ret'])
model = sm.OLS(Y, X).fit()
alpha, beta = model.params['const'], model.params['bench_ret']

# 5) annualized volatility (weekly data → √52)
ann_vol = strat_ret.std() * np.sqrt(52)

# 6) geometric mean return (annualized)
#    drop zero‐return periods to avoid gmean(1.0) bias
gm = gmean(1 + strat_ret[strat_ret != 0]) - 1
geo_ann = (1 + gm)**52 - 1

# 7) Sharpe ratio (zero RF)
sharpe = strat_ret.mean() / strat_ret.std() * np.sqrt(52)

# 8) average return per trade
avg_ret_trade = strat_ret.mean()

# 9) average number of trades per year
n_trades = (blotter['qty'] != 0).sum() # number of trades per year bc backtest period is one year


# 10) assemble into a Series and show
stats = pd.Series({
    'Alpha': alpha,
    'Beta': beta,
    'Annualized Volatility': ann_vol,
    'Geometric Mean Return': geo_ann,
    'Sharpe Ratio': sharpe,
    'Avg Return/Trade': avg_ret_trade,
    'Trades per Year': n_trades
})

show(stats.to_frame('Value').round(4))

```
